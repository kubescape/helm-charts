# Default values for the Kubescape operator chart


# -----------------------------------------------------------------------------------------
# ------------------------------- Basic ---------------------------------------------------
# -----------------------------------------------------------------------------------------

# cluster name: cluster name must be defined by the user!
# --set clusterName=`kubectl config current-context`
clusterName: "cluster"

# The namespace for running the chart
# If you wish to run the chart in a different namespace, make sure to update this value
ksNamespace: kubescape
ksLabel: kubescape

# global logger name and level
# the logger level and name are mapped to environment variables in each component
logger:
  level: info
  name: zap


# Additional volumes applied to all containers
volumes: [ ]

# Additional volumeMounts applied to all containers
volumeMounts: [ ]

customScheduling:
  affinity:
    # Specify affinity rules here
  nodeSelector:
    # Define nodeSelector rules here
  tolerations:
    # Set tolerations for nodes here

# -- set the image pull secrets for private registry support
imagePullSecrets: ""

# -----------------------------------------------------------------------------------------
# ----------------------------- Providers -------------------------------------------------
# -----------------------------------------------------------------------------------------

# Host and credentials for third-party providers
# Here you can find the list of providers -> https://kubescape.io/docs/providers/#compatible-providers
server:

# The chart will create a secret with the "account" and "access-key", in case you have a pre-created secret, use "credentials.cloudSecret" instead
account:
accessKey:

# When left blank, the secret will be generated using the default name. Override this option if you already have a secret and wish to prevent Helm from creating the default secret on your behalf
credentials:
  cloudSecret:

# -----------------------------------------------------------------------------------------
# --------------------------- Capabilities ------------------------------------------------
# -----------------------------------------------------------------------------------------

capabilities:
  # ====== configuration scanning related capabilities ======
  #
  # Default configuration scanning setup
  configurationScan: enable
  # Continuous Scanning continuously evaluates the security posture of your cluster.
  continuousScan: disable
  nodeScan: enable

  # ====== Image vulnerabilities scanning related capabilities ======
  #
  vulnerabilityScan: enable
  relevancy: enable
  # Generate VEX documents alongside the image vulnerabilities report (experimental)
  vexGeneration: disable

  # ====== Runtime related capabilities ======
  #
  runtimeObservability: enable
  networkPolicyService: enable
  runtimeDetection: disable
  malwareDetection: disable

  # ====== Other capabilities ======
  #
  # This is an experimental capability with an elevated security risk. Read the
  # matching docs before enabling.
  autoUpgrading: disable
  prometheusExporter: disable
  # seccompGenerator: disable

configurations:
  otelUrl: # default is empty
  persistence: enable

  # Enable Prometheus annotations, to allow open-source Prometheus (not operator) to scrape metrics
  prometheusAnnotations: disable

# installation of the alertCRD chart
alertCRD:
  installDefault: false # install the default CRD
  scopeClustered: false # it is better to have the CRDs in the cluster scope
  scopeNamespaced: false # enable scopeNamespaced when there are no permissions for creating cluster scoped CRDs

# -----------------------------------------------------------------------------------------
# ------------------------ Cloud Providers ------------------------------------------------
# -----------------------------------------------------------------------------------------

# It is recommended to setup the cloud info when installing the chart on a managed cluster, this will enable to check the related settings
# cloud support
cloudProviderMetadata:

  # -- cloud region
  cloudRegion:

  # -- AWS IAM arn role
  awsIamRoleArn:

  # -- GKE service account
  gkeServiceAccount:

  # -- GKE project
  gkeProject:

# -----------------------------------------------------------------------------------------
# ------------------------- Configurations ------------------------------------------------
# -----------------------------------------------------------------------------------------

# Enable persistence using Persistent Volume Claims
persistence:

  # -- persistence storage class
  #    ref: https://kubernetes.io/docs/concepts/storage/storage-classes/
  #    note: set to "-" (dash) for default storage class
  storageClass: "-"

  # -- persistence access mode
  #    ref: https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes
  accessMode: ReadWriteOnce

  # -- persistence size
  size:
    backingStorage: 5Gi
    kubevuln: 2Gi

global:
  httpsProxy: ""
  proxySecretFile: ""
  proxySecretName: kubescape-proxy-certificate
  namespaceTier: ks-control-plane
  cloudConfig: ks-cloud-config
  proxySecretDirectory: proxy-support
  configMapsDirectory: configs
  kubescapePsp:
    name: ks-allow-privileged
    enabled: false
  networkPolicy:
    enabled: false
    createEgressRules: false
  overrideRuntimePath: ""

# Image scanning configurations
imageScanning:
  # Provide credentials here when scanning images pulled from private container registries.
  # Note: When using imagePullSecrets this configuration is not necessary.
  #   ref: https://kubescape.io/docs/operator/vulnerabilities/#scanning-images-pulled-from-private-registries
  privateRegistries:
    credentials:
      # - registry: "<registry.example.com>"
      #   username: "<username/clientID>"
      #   password: "<password/clientSecret>"
      #   skipTlsVerify: false   # optional (skip TLS verification, default is false)
      #   insecure: false        # optional (use http instead of https, default is false)
      # Add more credentials as needed

# -----------------------------------------------------------------------------------------
# ----------------------- Microservices - core --------------------------------------------
# -----------------------------------------------------------------------------------------

# +++++++++++++++++++++++++++++++ Kubescape ++++++++++++++++++++++++++++++++++++++++++++++++

# kubescape scanner - https://github.com/kubescape/kubescape
kubescape:

  name: kubescape

  image:
    # -- source code: https://github.com/kubescape/kubescape/tree/master/httphandler (public repo)
    repository: quay.io/kubescape/kubescape
    tag: v3.0.9
    pullPolicy: IfNotPresent

  resources:
    requests:
      cpu: 250m
      memory: 400Mi
    limits:
      cpu: 600m
      memory: 1Gi

  # -- download policies every scan, we recommend it should remain true, you should change to 'false' when running in an air-gapped environment or when scanning with high frequency (when running with Prometheus)
  downloadArtifacts: true

  # -- skip check for a newer version
  skipUpdateCheck: false

  # -- submit results to the Kubescape cloud: https://cloud.armosec.io/
  submit: true

  service:
    type: ClusterIP
    port: 8080

  # deploy a service monitor for prometheus (operator) integration
  serviceMonitor:
    # -- enable/disable service monitor for prometheus (operator) integration
    enabled: false

    # -- Customize prometheus interval and scrapeTimeout
    interval: 200s
    scrapeTimeout: 150s

    # If needed the service monitor can be deployed to a different namespace than the one kubescape is in
    #namespace: my-namespace

  # Additional volumes to be mounted on Kubescape
  volumes: [ ]

  # Additional volumeMounts to be mounted on Kubescape
  volumeMounts: [ ]

# +++++++++++++++++++++++++++++++ Operator ++++++++++++++++++++++++++++++++++++++++++++++++

# Operator will trigger kubescape and kubevuln scanning
operator:

  # operator Deployment name
  name: operator

  image:
    # -- source code: https://github.com/kubescape/operator
    repository: quay.io/kubescape/operator
    tag: v0.2.9
    pullPolicy: IfNotPresent

  service:
    type: ClusterIP
    port: 4002
    targetPort: 4002
    protocol: TCP

  resources:
    requests:
      cpu: 50m
      memory: 100Mi
    limits:
      cpu: 300m
      memory: 300Mi
  env: { }
  labels: { }

  # Additional volumes to be mounted on the websocket
  volumes: [ ]

  # Additional volumeMounts to be mounted on the websocket
  volumeMounts: [ ]

  triggerSecurityFramework: true

# +++++++++++++++++++++++++++++++ Kubevuln ++++++++++++++++++++++++++++++++++++++++++++++++

# kubevuln - image vulnerability scanning microservice
kubevuln:

  # -- for enable:"<any-value>", for disable:"": the print of json posted to the Kubescape cloud from the vuln scanner
  verbose: ""

  # kubevuln Deployment name
  name: kubevuln

  image:
    # -- source code: https://github.com/kubescape/kubevuln
    repository: quay.io/kubescape/kubevuln
    tag: v0.3.18
    pullPolicy: IfNotPresent

  replicaCount: 1

  service:
    type: ClusterIP
    port: 8080
    targetPort: 8080
    protocol: TCP

  resources:
    requests:
      cpu: 300m
      memory: 1000Mi
      # Consider to increase ephemeral-storage requests in order to avoid pod eviction due to huge images
      # More details: https://hub.armosec.io/docs/limitations
      #               https://github.com/kubescape/kubescape/issues/389
      ephemeral-storage: 5Gi
    limits:
      cpu: 1500m
      memory: 5000Mi
      ephemeral-storage: 6Gi
  config:
    maxImageSize: 5368709120 # set max image size for scanning, It is recommended to use the same as the requested ephemeral-storage
    scanTimeout: 5m # set timeout for scanning an image

  env:
    - name: CA_MAX_VULN_SCAN_ROUTINES # TODO update the kubevuln
      value: "1"

  labels: { }

  # Additional volumes to be mounted on the vulnerability scanning microservice
  volumes: [ ]

  # Additional volumeMounts to be mounted on the vulnerability scanning microservice
  volumeMounts: [ ]

# +++++++++++++++++++++++++++++++ Kollector ++++++++++++++++++++++++++++++++++++++++++++++++

# kollector will collect the data only in the kubescape namespace and report the data to the Kubescape cloud. This is to enable onDemand scanning and for creating/editing/deleting scheduled scans from the Kubescape cloud
kollector:

  # kollector SS name
  name: kollector

  image:
    # -- source code: https://github.com/kubescape/kollector
    repository: quay.io/kubescape/kollector
    tag: v0.1.36
    pullPolicy: IfNotPresent

  resources:
    requests:
      cpu: 10m
      memory: 100Mi
    limits:
      cpu: 500m
      memory: 500Mi


  env: {}

  labels: { }

  # Additional volumes to be mounted on the collector
  volumes: [ ]

  # Additional volumeMounts to be mounted on the collector
  volumeMounts: [ ]

# +++++++++++++++++++++++++++++++ Gateway ++++++++++++++++++++++++++++++++++++++++++++++++

# gateway pass notifications from Kubescape cloud to the Operator microservice. The notifications are the onDemand scanning and the scanning schedule settings
gateway:

  # gateway Deployment name
  name: gateway

  websocketService:
    type: ClusterIP
    port: 8001
    targetPort: 8001
    protocol: TCP

  httpService:
    type: ClusterIP
    port: 8002
    targetPort: 8002
    protocol: TCP
  image:
    # -- source code: https://github.com/kubescape/gateway
    repository: quay.io/kubescape/gateway
    tag: v0.1.23
    pullPolicy: IfNotPresent

  resources:
    requests:
      cpu: 10m
      memory: 30Mi
    limits:
      cpu: 100m
      memory: 50Mi

  env: { }
  labels: { }

  # Additional volumes to be mounted on the notification-service
  volumes: [ ]

  # Additional volumeMounts to be mounted on the notification-service
  volumeMounts: [ ]

# +++++++++++++++++++++++++++++++ Host-scanner ++++++++++++++++++++++++++++++++++++++++++++++++

hostScanner:
  image:
    # -- source code: https://github.com/kubescape/host-scanner (public repo)
    repository: quay.io/kubescape/host-scanner
    tag: v1.0.66
    pullPolicy: IfNotPresent

  # Additional volumes to be mounted on the Kubescape host scanner
  volumes: [ ]

  # Additional volumeMounts to be mounted on the Kubescape host scanner
  volumeMounts: [ ]

  tolerations:
    # this toleration is to have the DaemonDet runnable on master nodes
    # remove it if your masters can't run pods
    - key: node-role.kubernetes.io/control-plane
      operator: Exists
      effect: NoSchedule
    - key: node-role.kubernetes.io/master
      operator: Exists
      effect: NoSchedule

# +++++++++++++++++++++++++++++++ Storage ++++++++++++++++++++++++++++++++++++++++++++++++

# Values for the Kubescape Storage service that Kubescape uses for its internal
# purposes like storage
storage:

  # Values or the Aggregated APIServer
  name: "storage"

  image:
    # -- source code: https://github.com/kubescape/storage
    repository: quay.io/kubescape/storage
    tag: v0.0.83
    pullPolicy: IfNotPresent

  # cleanup interval is a duration string
  cleanupInterval: "6h"

  labels:
    app.kubernetes.io/name: "storage"
    app.kubernetes.io/component: "apiserver"
    app.kubernetes.io/part-of: "kubescape-storage"

  resources:
    requests:
      cpu: 100m
      memory: 400Mi
    limits:
      cpu: 500m
      memory: 1500Mi

# +++++++++++++++++++++++++++++ Node-agent ++++++++++++++++++++++++++++++++++++++++++++++++

nodeAgent:
  name: node-agent
  image:
    # -- source code: https://github.com/kubescape/node-agent
    repository: quay.io/kubescape/node-agent
    tag: v0.2.56
    pullPolicy: IfNotPresent

  config:
    maxLearningPeriod: 12h # duration string
    learningPeriod: 2m # duration string
    updatePeriod: 10m # duration string
    prometheusExporter: disable
    httpExporterConfig:
      url: http://synchronizer:8089/apis/v1/kubescape.io/v1/runtimealerts
      maxAlertsPerMinute: 1000
    alertManagerExporterUrls: []
    stdoutExporter: true
    syslogExporterURL: ""

  # prometheus (operator) service monitor
  serviceMonitor:
    # -- enable/disable service monitor for prometheus
    enabled: false

    # -- Customize prometheus interval and scrapeTimeout
    interval: 30s
    scrapeTimeout: 15s

  resources:
    requests:
      cpu: 100m
      memory: 180Mi
    limits:
      cpu: 500m
      memory: 700Mi

  env:
    - name: NodeName
      valueFrom:
        fieldRef:
          fieldPath: spec.nodeName

  privileged: false
  seLinuxType: spc_t

  volumeMounts:
    - mountPath: /host
      name: host
    - mountPath: /run
      name: run
    - mountPath: /lib/modules
      name: modules
    - mountPath: /sys/kernel/debug
      name: debugfs
    - mountPath: /sys/fs/cgroup
      name: cgroup
    - mountPath: /sys/fs/bpf
      name: bpffs
    - mountPath: /data
      name: data

  volumes:
    - hostPath:
        path: /
      name: host
    - hostPath:
        path: /run
      name: run
    - hostPath:
        path: /sys/fs/cgroup
      name: cgroup
    - hostPath:
        path: /lib/modules
      name: modules
    - hostPath:
        path: /sys/fs/bpf
      name: bpffs
    - hostPath:
        path: /sys/kernel/debug
      name: debugfs
    - emptyDir:
      name: data

  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
              - key: kubernetes.io/os
                operator: In
                values:
                  - linux

  nodeSelector:
    kubernetes.io/os: linux

# +++++++++++++++++++++++++++++++ ClamAV ++++++++++++++++++++++++++++++++++++++++++++++++
clamav:
  name: clamav
  image:
    repository: quay.io/armosec/klamav
    tag: beta5
    pullPolicy: Always

  resources:
    limits:
      cpu: 300m
      memory: 512Mi
    requests:
      cpu: 100m
      memory: 256Mi

  volumeMounts:
    - mountPath: /var/lib/clamav-tmp
      name: clamdb
      readOnly: false
    - mountPath: /etc/clamav
      name: etc
      readOnly: true

  volumes:
  - name: clamdb
    emptyDir: {}
  - name: etc
    configMap:
      name: clamav
      items:
      - key: clamd.conf
        path: clamd.conf
      - key: freshclam.conf
        path: freshclam.conf

  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
              - key: kubernetes.io/os
                operator: In
                values:
                  - linux

  nodeSelector:
    kubernetes.io/os: linux
# +++++++++++++++++++++++++++++ Synchronizer ++++++++++++++++++++++++++++++++++++++++++++++++

synchronizer:
  name: synchronizer
  image:
    # -- source code: https://github.com/kubescape/synchronizer
    repository: quay.io/kubescape/synchronizer
    tag: v0.0.72
    pullPolicy: IfNotPresent
  resources:
    requests:
      cpu: 100m
      memory: 250Mi
    limits:
      cpu: 200m
      memory: 500Mi
  service:
    type: ClusterIP
    port: 8089
    targetPort: 8089
    protocol: TCP

# -----------------------------------------------------------------------------------------
# ------------------------ Microservice - helpers -----------------------------------------
# -----------------------------------------------------------------------------------------


# +++++++++++++++++++++++++++++++ OTEL-collector ++++++++++++++++++++++++++++++++++++++++++++++++

# opentelemetry collector
otelCollector:
  name: otel-collector

  endpoint:
    insecure: true
    headers:
      uptrace-dsn: ""

  # -- enable/disable hostmetrics collection
  hostmetrics:
    enabled: true
    scrapeInterval: 30s

  image:
    repository: docker.io/otel/opentelemetry-collector
    tag: 0.97.0
    pullPolicy: IfNotPresent

  resources:
    requests:
      cpu: 100m
      memory: 500Mi
    limits:
      cpu: 1
      memory: 1Gi

# +++++++++++++++++++++++++++++ GrypeOfflineDB ++++++++++++++++++++++++++++++++++++++++++++++++

grypeOfflineDB:
  enabled: false

  name: grype-offline-db

  image:
    repository: ghcr.io/alexandreroman/grype-offline-db
    sha: sha256:155db3be4baa461a50cebadfc8f52108fca71aa4ce5e460a30a4e0922e899ed2
    pullPolicy: IfNotPresent

  resources:
    requests:
      cpu: 150m
      memory: 200Mi
    limits:
      cpu: 150m
      memory: 200Mi

# +++++++++++++++++++++++++++++ Discovery ++++++++++++++++++++++++++++++++++++++++++++++++

# service discovery job for discovering backend server URLs
serviceDiscovery:
  name: service-discovery

  metrics: ""
  services: ""

  urlDiscovery:
    name: url-discovery
    image:
      repository: quay.io/kubescape/http-request
      tag: v0.2.6
      pullPolicy: IfNotPresent

  configMapCheck:
    name: check-url-configmap
    image:
      repository: docker.io/bitnami/kubectl
      tag: 1.27.6
      pullPolicy: IfNotPresent

  configMapUpdate:
    name: update-configmap
    image:
      repository: docker.io/bitnami/kubectl
      tag: 1.27.6
      pullPolicy: IfNotPresent

  resources:
    requests:
      cpu: 10m
      memory: 10Mi
    limits:
      cpu: 100m
      memory: 50Mi

# +++++++++++++++++++++++++++++ Prometheus exporter ++++++++++++++++++++++++++++++++++++++++++++++++

# Prometheus exporter
prometheusExporter:
  name: "prometheus-exporter"
  image:
    repository: quay.io/kubescape/prometheus-exporter
    tag: v0.0.7
    pullPolicy: IfNotPresent

  resources:
    requests:
      cpu: 10m
      memory: 10Mi
    limits:
      cpu: 50m
      memory: 100Mi

  service:
    port: 8080
    targetPort: 8080
    protocol: TCP

# +++++++++++++++++++++++++++++ Upgrader ++++++++++++++++++++++++++++++++++++++++++++++++

# Configures the Helm Release Upgrader
helmReleaseUpgrader:
  name: "helm-release-upgrader"
  image:
    repository: quay.io/kubescape/helm-release-upgrader
    tag: v0.1.0
    pullPolicy: IfNotPresent

  # A cron schedule of how often the updating CronJob should run
  schedule: "0 14 * * *"

  # Resource requests and limits for the CronJob
  resources:
    # Requests and Limits are the same to make the CronJob Burstable
    requests:
      # Setting a higher CPU request helps with the Job runtime. If you don’t
      # care about job execution speed and want to save on resources, feel free
      # to lower this
      cpu: 500m
      memory: 256Mi
    limits:
      cpu: 500m
      # Keep the memory limit sufficiently high.
      #
      # The updating CronJob runs an image that runs `helm upgrade`. It renders
      # the chart and that can require a lot of memory. If you don’t want your
      # updating job to be OOM Killed, keep this at 256 MiB or higher depending
      # on the size of your cluster.
      memory: 256Mi

  successfulJobsHistoryLimit: 3
  failedJobsHistoryLimit: 1

# -----------------------------------------------------------------------------------------
# --------------------------- Schedulers --------------------------------------------------
# -----------------------------------------------------------------------------------------

# kubescape scheduled scan using a CronJob
kubescapeScheduler:

  # scan scheduler container name
  name: kubescape-scheduler

    # -- Frequency of running the scan
    #     ┌───────────── minute (0 - 59)
    #     │ ┌───────────── hour (0 - 23)
    #     │ │ ┌───────────── day of the month (1 - 31)
    #     │ │ │ ┌───────────── month (1 - 12)
    #     │ │ │ │ ┌───────────── day of the week (0 - 6) (Sunday to Saturday;
    #     │ │ │ │ │                         7 is also Sunday on some systems)
    #     │ │ │ │ │
    #     │ │ │ │ │
  #     * * * * *
  # -- scan schedule frequency
  scanSchedule: "0 8 * * *"
  image:
    # -- source code: https://github.com/kubescape/http-request (public repo)
    repository: quay.io/kubescape/http-request
    tag: v0.2.6
    pullPolicy: IfNotPresent

  # Additional volumes to be mounted on the scan scheduler
  volumes: [ ]

  # Additional volumeMounts to be mounted on the scan scheduler
  volumeMounts: [ ]

  resources:
    requests:
      cpu: 1m
      memory: 10Mi
    limits:
      cpu: 10m
      memory: 20Mi

  successfulJobsHistoryLimit: 3
  failedJobsHistoryLimit: 1


kubevulnScheduler:

  ## scan scheduler container name
  name: kubevuln-scheduler

    # -- Frequency of running the scan
    #     ┌───────────── minute (0 - 59)
    #     │ ┌───────────── hour (0 - 23)
    #     │ │ ┌───────────── day of the month (1 - 31)
    #     │ │ │ ┌───────────── month (1 - 12)
    #     │ │ │ │ ┌───────────── day of the week (0 - 6) (Sunday to Saturday;
    #     │ │ │ │ │                         7 is also Sunday on some systems)
    #     │ │ │ │ │
    #     │ │ │ │ │
  #     * * * * *
  scanSchedule: "0 0 * * *"
  image:
    # source code - https://github.com/kubescape/http-request
    repository: quay.io/kubescape/http-request
    tag: v0.2.6
    pullPolicy: IfNotPresent

  # Additional volumes to be mounted on the vuln scan scheduler
  volumes: [ ]

  # Additional volumeMounts to be mounted on the vuln scan scheduler
  volumeMounts: [ ]

  resources:
    requests:
      cpu: 1m
      memory: 10Mi
    limits:
      cpu: 10m
      memory: 20Mi

  successfulJobsHistoryLimit: 3
  failedJobsHistoryLimit: 1


# registry scan scheduled scan using a CronJob
registryScanScheduler:

  # scan scheduler container name
  name: registry-scheduler

    # -- Frequency of running the scan
    #     ┌───────────── minute (0 - 59)
    #     │ ┌───────────── hour (0 - 23)
    #     │ │ ┌───────────── day of the month (1 - 31)
    #     │ │ │ ┌───────────── month (1 - 12)
    #     │ │ │ │ ┌───────────── day of the week (0 - 6) (Sunday to Saturday;
    #     │ │ │ │ │                         7 is also Sunday on some systems)
    #     │ │ │ │ │
    #     │ │ │ │ │
  #     * * * * *
  # -- scan schedule frequency
  scanSchedule: "0 0 * * *"
  image:
    # -- source code: https://github.com/kubescape/http-request (public repo)
    repository: quay.io/kubescape/http-request
    tag: v0.2.6
    pullPolicy: IfNotPresent

  # Additional volumes to be mounted on the scan scheduler
  volumes: [ ]

  # Additional volumeMounts to be mounted on the scan scheduler
  volumeMounts: [ ]

  resources:
    requests:
      cpu: 1m
      memory: 10Mi
    limits:
      cpu: 10m
      memory: 20Mi

  successfulJobsHistoryLimit: 3
  failedJobsHistoryLimit: 1

# -----------------------------------------------------------------------------------------
# ------------------------- Configurations ------------------------------------------------
# -----------------------------------------------------------------------------------------

# Continuous scanning configurations
continuousScanning:
  configMapName: cs-matching-rules

  # Matching rules for the monitored resources.
  # Kubescape will watch resources of every provided GVR across the provided
  # namespaces.
  matchingRules:
    match:
      - apiGroups: ["apps"]
        apiVersions: ["v1"]
        resources: ["deployments"]
    namespaces:
      - default
