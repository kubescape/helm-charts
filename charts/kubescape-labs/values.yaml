# Default values for KS chart.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

ksNamespace: kubescape
ksLabel: kubescape
createKubescapeServiceAccount: true # TODO: move to kubescape

# KS cloud BE URLs
environment: "prod"
eventReceiverHttpUrl: "https://report.armo.cloud"
k8sReportUrl: "wss://report.armo.cloud"
gatewayUrl: "ens.euprod1.cyberarmorsoft.com"
backendOpenAPI: "https://api.armosec.io/api"
ksCloudOtelCollector: "otelcol.armosec.io:443"
# KS cloud DEV BE URLs
devEventReceiverHttpUrl: "https://report.eudev3.cyberarmorsoft.com"
devK8sReportUrl: "wss://report.eudev3.cyberarmorsoft.com"
devGatewayUrl: "ens.eudev3.cyberarmorsoft.com"
devBackendOpenAPI: "https://api-dev.armosec.io/api"
devKsCloudOtelCollector: "otelcol-dev.armosec.io:443"
# KS cloud STAGING BE URLs
stagingEventReceiverHttpUrl: "https://report-ks.eustage2.cyberarmorsoft.com"
stagingK8sReportUrl: "wss://report.eustage2.cyberarmorsoft.com"
stagingGatewayUrl: "ens.eustage2.cyberarmorsoft.com" 
stagingBackendOpenAPI: "https://api-stage.armosec.io/api"
# Customer Specific Data
# account is deliberately not defined here and it should be defined by the user
clusterName: # cluster name must be defined by the user

# -- client ID - https://hub.armosec.io/docs/authentication
clientID: ""

# -- secret key - https://hub.armosec.io/docs/authentication
secretKey: ""

# -- set the image pull secrets for private registry support 
imagePullSecrets: ""

logger:
  level: info
  name: zap

# cloud support
cloudProviderMetadata:

  # -- cloud provider engine
  cloudProviderEngine:

  # -- cloud region
  cloudRegion:

  # -- AWS IAM arn role
  awsIamRoleArn:

  # -- GKE service account
  gkeServiceAccount:

  # -- GKE project
  gkeProject:

# -- enable/disable trigger image scan for new images
triggerNewImageScan: false  

# Additional volumes applied to all containers
volumes: []

# Additional volumeMounts applied to all containers
volumeMounts: []

global:
  namespaceTier: ks-control-plane
  cloudConfig: ks-cloud-config
  operatorServiceAccountName: ks-sa # TODO - split service account to the different components
  kubescapeServiceAccountName: kubescape-sa # TODO - split service account to the different components
  networkPolicy:
    enabled: false
    createEgressRules: false

  # -- enable/disable revision label
  addRevisionLabel: true

  # -- submit results to the ARMO portal. Default is true
  keepLocal: false

# kubescape scheduled scan using a CronJob
kubescapeScheduler: 

  # -- enable/disable a kubescape scheduled scan using a CronJob
  enabled: true

  # scan scheduler container name
  name: kubescape-scheduler

           # -- Frequency of running the scan
           #     ┌───────────── minute (0 - 59)
           #     │ ┌───────────── hour (0 - 23)
           #     │ │ ┌───────────── day of the month (1 - 31)
           #     │ │ │ ┌───────────── month (1 - 12)
           #     │ │ │ │ ┌───────────── day of the week (0 - 6) (Sunday to Saturday;
           #     │ │ │ │ │                         7 is also Sunday on some systems)
           #     │ │ │ │ │
           #     │ │ │ │ │
           #     * * * * *
  # -- scan schedule frequency
  scanSchedule: "0 8 * * *"
  image:
    # -- source code: https://github.com/kubescape/http-request (public repo)
    repository: quay.io/kubescape/http-request
    tag: 	v0.0.14
    pullPolicy: IfNotPresent

  replicaCount: 1

  # Additional volumes to be mounted on the scan scheduler
  volumes: []

  # Additional volumeMounts to be mounted on the scan scheduler
  volumeMounts: []

  resources:
    requests:
       cpu: 1m
       memory: 2Mi
    limits:
       cpu: 10m
       memory: 6Mi

# kubescape scanner - https://github.com/kubescape/kubescape
kubescape:

  # -- enable/disable kubescape scanning
  enabled: true

  name: kubescape

  image:
    # -- source code: https://github.com/kubescape/kubescape/tree/master/httphandler (public repo)
    repository: quay.io/dwertent/kubescape
    tag: v3.1.9
    pullPolicy: Always

  resources:
    requests:
       cpu: 250m
       memory: 400Mi
    limits:
       cpu: 600m
       memory: 800Mi

  # -- enable host scanner feature: https://hub.armosec.io/docs/host-sensor
  enableHostScan: true

  # -- download policies every scan, we recommend it should remain true, you should change to 'false' when running in an air-gapped environment or when scanning with high frequency (when running with Prometheus)
  downloadArtifacts: true

  # -- skip check for a newer version 
  skipUpdateCheck: false

  # -- submit results to the Kubescape cloud: https://cloud.armosec.io/
  submit: true

  replicaCount: 1

  service:
    type: ClusterIP
    port: 8080

  # deploy a service monitor for prometheus (operator) integration
  serviceMonitor:
    # -- enable/disable service monitor for prometheus (operator) integration
    enabled: false

    # If needed the service monitor can be deployed to a different namespace than the one kubescape is in
    #namespace: my-namespace

  # Additional volumes to be mounted on Kubescape
  volumes: []

  # Additional volumeMounts to be mounted on Kubescape
  volumeMounts: []

# Operator will trigger kubescape and kubevuln scanning
operator:

  # -- enable/disable Operator
  enabled: true

  replicaCount: 1

  # operator Deployment name
  name: operator

  image:
    # -- source code: https://github.com/kubescape/operator
    repository: quay.io/kubescape/operator
    tag: 	v0.2.44-relevancy
    pullPolicy: Always

  service:
    type: ClusterIP
    port: 4002
    targetPort: 4002
    protocol: TCP

  resources:
    requests:
       cpu: 50m
       memory: 100Mi
    limits:
       cpu: 300m
       memory: 300Mi
  env: {}
  labels: {}

  # Additional volumes to be mounted on the websocket
  volumes: []

  # Additional volumeMounts to be mounted on the websocket
  volumeMounts: []

kubevulnScheduler:

  ## Schedule Scan using cron
  enabled: true

  ## scan scheduler container name
  name: kubevuln-scheduler

           # -- Frequency of running the scan
           #     ┌───────────── minute (0 - 59)
           #     │ ┌───────────── hour (0 - 23)
           #     │ │ ┌───────────── day of the month (1 - 31)
           #     │ │ │ ┌───────────── month (1 - 12)
           #     │ │ │ │ ┌───────────── day of the week (0 - 6) (Sunday to Saturday;
           #     │ │ │ │ │                         7 is also Sunday on some systems)
           #     │ │ │ │ │
           #     │ │ │ │ │
           #     * * * * *
  scanSchedule: "0 0 * * *"
  image:
    # source code - https://github.com/kubescape/http-request
    repository: quay.io/kubescape/http-request
    tag: 	v0.0.14
    pullPolicy: IfNotPresent

  replicaCount: 1

  # Additional volumes to be mounted on the vuln scan scheduler
  volumes: []

  # Additional volumeMounts to be mounted on the vuln scan scheduler
  volumeMounts: []

  resources:
    requests:
       cpu: 1m
       memory: 2Mi
    limits:
       cpu: 10m
       memory: 6Mi

# kubevuln - image vulnerability scanning microservice
kubevuln:

  # -- for enable:"<any-value>", for disable:"": the print of json posted to the Kubescape cloud from the vuln scanner 
  verbose: ""

  # -- enable/disable kubevuln
  enabled: true

  # kubevuln Deployment name
  name: kubevuln

  image:
    # -- source code: https://github.com/kubescape/kubevuln
    repository: quay.io/kubescape/kubevuln
    tag: 	v0.2.72-relevancy
    pullPolicy: Always

  replicaCount: 1

  service:
    type: ClusterIP
    port: 8080
    targetPort: 8080
    protocol: TCP

  resources:
    requests:
       cpu: 300m
       memory: 1000Mi
       # Consider to increase ephemeral-storage requests in order to avoid pod eviction due to huge images
       # More details: https://hub.armosec.io/docs/limitations
       #               https://github.com/kubescape/kubescape/issues/389
       ephemeral-storage: 5Gi
    limits:
       cpu: 500m
       memory: 5000Mi
       ephemeral-storage: 6Gi
  
  config:
    maxImageSize: 5368709120 # set max image size for scanning, It is recommended to use the same as the requested ephemeral-storage

  env:
  - name: CA_MAX_VULN_SCAN_ROUTINES # TODO update the kubevuln
    value: "1"

  labels: {}

  # Additional volumes to be mounted on the vulnerability scanning microservice
  volumes: []

  # Additional volumeMounts to be mounted on the vulnerability scanning microservice
  volumeMounts: []

# kollector will collect the data only in the kubescape namespace and report the data to the Kubescape cloud. This is to enable onDemand scanning and for creating/editing/deleting scheduled scans from the Kubescape cloud
kollector:

  # -- enable/disable the kollector
  enabled: true

  # kollector SS name
  name: kollector

  image:
    # -- source code: https://github.com/kubescape/kollector
    repository: quay.io/kubescape/kollector
    tag: v0.1.12
    pullPolicy: Always

  replicaCount: 1

  resources:
    requests:
       cpu: 10m
       memory: 40Mi
    limits:
       cpu: 500m
       memory: 500Mi


  env:
  # -- print in verbose mode (print all reported data)
  - name: PRINT_REPORT
    value: "false"

  # -- wait before first report
  - name: WAIT_BEFORE_REPORT
    value: "0"

  labels: {}

  # Additional volumes to be mounted on the collector
  volumes: []

  # Additional volumeMounts to be mounted on the collector
  volumeMounts: []

# gateway pass notifications from Kubescape cloud to the Operator microservice. The notifications are the onDemand scanning and the scanning schedule settings
gateway:

  # -- enable/disable passing notifications from Kubescape cloud to the Operator microservice. The notifications are the onDemand scanning and the scanning schedule settings
  enabled: true

  # gateway Deployment name
  name: gateway

  websocketService:
    type: ClusterIP
    port: 8001
    targetPort: 8001
    protocol: TCP

  httpService:
    type: ClusterIP
    port: 8002
    targetPort: 8002
    protocol: TCP
  image:
    # -- source code: https://github.com/kubescape/gateway
    repository: quay.io/kubescape/gateway
    tag: v0.1.11
    pullPolicy: Always

  replicaCount: 1
  resources:
    requests:
       cpu: 10m
       memory: 10Mi
    limits:
       cpu: 100m
       memory: 50Mi

  env: {}
  labels: {}

  # Additional volumes to be mounted on the notification-service
  volumes: []

  # Additional volumeMounts to be mounted on the notification-service
  volumeMounts: []

kubescapeHostScanner:
  # Additional volumes to be mounted on the Kubescape host scanner
  volumes: []

  # Additional volumeMounts to be mounted on the Kubescape host scanner
  volumeMounts: []

# registry scan scheduled scan using a CronJob
registryScanScheduler:

  # -- enable/disable a kubescape scheduled scan using a CronJob
  enabled: true

  # scan scheduler container name
  name: registry-scheduler

           # -- Frequency of running the scan
           #     ┌───────────── minute (0 - 59)
           #     │ ┌───────────── hour (0 - 23)
           #     │ │ ┌───────────── day of the month (1 - 31)
           #     │ │ │ ┌───────────── month (1 - 12)
           #     │ │ │ │ ┌───────────── day of the week (0 - 6) (Sunday to Saturday;
           #     │ │ │ │ │                         7 is also Sunday on some systems)
           #     │ │ │ │ │
           #     │ │ │ │ │
           #     * * * * *
  # -- scan schedule frequency
  scanSchedule: "0 0 * * *"
  image:
    # -- source code: https://github.com/kubescape/http-request (public repo)
    repository: quay.io/kubescape/http-request
    tag: 	v0.0.14
    pullPolicy: IfNotPresent

  replicaCount: 1

  # Additional volumes to be mounted on the scan scheduler
  volumes: []

  # Additional volumeMounts to be mounted on the scan scheduler
  volumeMounts: []

  resources:
    requests:
       cpu: 1m
       memory: 2Mi
    limits:
       cpu: 10m
       memory: 6Mi

# opentelemetry collector
otelCollector:

  # -- enable/disable metrics and traces collection

  enabled: true
  endpoint:
    host: ""
    port: 4317
    insecure: true
    headers:
      uptrace-dsn: ""

  # -- enable/disable hostmetrics collection  
  hostmetrics:
    enabled: true
    scrapeInterval: 30s

  image:
    repository: otel/opentelemetry-collector
    tag: 0.70.0
    pullPolicy: Always

  replicaCount: 1

  resources:
    requests:
       cpu: 200m
       memory: 400Mi
    limits:
       cpu: 1
       memory: 2Gi

# Values for the Kubescape Storage service that Kubescape uses for its internal
# purposes like storage
kubescapeStorage:
  enabled: true

  # Values or the Aggregated APIServer
  k8sApiserver:
    name: "storage-aggregated-apiserver"

    deployment:
      labels:
        app.kubernetes.io/name: "aggregated-apiserver"
        app.kubernetes.io/component: "apiserver"
        app.kubernetes.io/part-of: "kubescape-storage"

      replicaCount: 1
      image:
        pullPolicy: "IfNotPresent"
        repository: "vklokun/kube-sample-apiserver"
        tag: "0.1.40"


  # Values for the storage that backs the Aggregated APIServer
  backingStorage:
    deployment: 

      labels:
        app.kubernetes.io/name: "etcd"
        app.kubernetes.io/component: "database"
        app.kubernetes.io/part-of: "kubescape-aa-storage"

      image:
        pullPolicy: "IfNotPresent"
        repository: "gcr.io/etcd-development/etcd"
        tag: "v3.5.7"
  
nodeAgent:
  enabled: true
  ServiceAccount: node-agent-service-account
  ClusterRole: node-agent-cluster-role
  ClusterRoleBinding: node-agent-cluster-role-binding
  config: 
    name: node-agent-config-map
    maxSniffingTime: 360 # minutes
    updateDataPeriod: 60 # seconds

  daemonset: 
    name: node-agent

  volumes:
    - name: configmap-volume
      configMap:
        name: node-agent-config-map
    - emptyDir: {}
      name: root-falco-fs
    - hostPath:
        path: /boot
        type: ""
      name: boot-fs
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - hostPath:
        path: /usr
        type: ""
      name: usr-fs
    - hostPath:
        path: /etc
        type: ""
      name: etc-fs
    - hostPath:
        path: /dev
        type: ""
      name: dev-fs
    - hostPath:
        path: /var/run/docker.sock
        type: ""
      name: docker-socket
    - hostPath:
        path: /run/containerd/containerd.sock
        type: ""
      name: containerd-socket
    - hostPath:
        path: /run/crio/crio.sock
        type: ""
      name: crio-socket
    - hostPath:
        path: /proc
        type: ""
      name: proc-fs
    - hostPath:
        path: /sys/kernel/debug
        type: ""
      name: debugfs

  initContainer:
    name: falco-driver-loader
    image:
      repository: docker.io/falcosecurity/falco-driver-loader
      tag: 0.32.2
      pullPolicy: IfNotPresent
    env:
      - name: FALCO_BPF_PROBE
    volumeMounts:
      - mountPath: /root/.falco
        name: root-falco-fs
        readOnly: false
      - mountPath: /host/proc
        name: proc-fs
        readOnly: true
      - mountPath: /host/boot
        name: boot-fs
        readOnly: true
      - mountPath: /host/lib/modules
        name: lib-modules
      - mountPath: /host/usr
        name: usr-fs
        readOnly: true
      - mountPath: /host/etc
        name: etc-fs
        readOnly: true
    
  containers:  
    nodeAgent:
      name: node-agent
      image: 
        repository: quay.io/kubescape/sniffer
        tag: v0.1.39-relevancy
        pullPolicy: IfNotPresent
      env:  
        - name: CONFIG_ENV_VAR
          value: "/etc/node-agent/configuration/ConfigurationFile.json"
        - name: NodeName
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
      volumeMounts:
        - mountPath: /etc/node-agent/configuration
          name: configmap-volume
        - mountPath: /root/.falco
          name: root-falco-fs
        - mountPath: /host/proc
          name: proc-fs
        - mountPath: /sys/kernel/debug
          name: debugfs
        - mountPath: /host/var/run/docker.sock
          name: docker-socket
        - mountPath: /host/run/containerd/containerd.sock
          name: containerd-socket
        - mountPath: /host/run/crio/crio.sock
          name: crio-socket
